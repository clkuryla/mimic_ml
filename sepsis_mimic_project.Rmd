---
title: "ML MIMIC Sepsis Project"
author: "Christine Lucille Kuryla"
date: "2024-12-07"
output: 
  html_document:
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)


library(tidyverse)
library(xgboost)
library(MASS)       # for stepAIC
library(caret)      # for creating dummy variables and data splitting
library(lubridate)  # if needed for date manipulation

library(conflicted)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")


```

# Article

Hou, N., Li, M., He, L. et al. Predicting 30-days mortality for MIMIC-III patients with sepsis-3: a machine learning approach using XGboost. J Transl Med 18, 462 (2020). https://doi.org/10.1186/s12967-020-02620-5

https://translational-medicine.biomedcentral.com/articles/10.1186/s12967-020-02620-5 

# Data from online article

## Load Data
```{r load_data}
# Downloaded from: https://translational-medicine.biomedcentral.com/articles/10.1186/s12967-020-02620-5#Sec14

# This data has all of the patients included in the study, after they filtered out certain patients due to the exclusion criteria in the paper
# n = 4559

data <- read_csv("data/12967_2020_2620_MOESM1_ESM.csv") 

# colnames(data)

# Remove duplicates

# all(data$icustay_id...1 == data$icustay_id...103) # TRUE
# all(data$hadm_id...102 == data$hadm_id...2) # TRUE

data <- data %>% 
  mutate(icustay_id = icustay_id...1,
         hadm_id = hadm_id...2
         ) %>% 
  select(-c("icustay_id...1", "icustay_id...103",
            "hadm_id...2", "hadm_id...102"
            ))

# Some methods need numeric data, some need different variable types. Let's create two dataframes.

data_numeric <- data

# Convert into appropriate data types

data_vartypes <- data %>% 
  mutate(intime = dmy_hms(intime),
         outtime = dmy_hms(outtime),
         dbsource = as_factor(dbsource),
         suspected_infection_time_poe = dmy_hms(suspected_infection_time_poe),
         specimen_poe = as_factor(specimen_poe),
         positiveculture_poe = as_factor(positiveculture_poe),
         antibiotic_time_poe = dmy_hms(antibiotic_time_poe),
         blood_culture_time = dmy_hms(blood_culture_time),
         blood_culture_positive = as_factor(blood_culture_positive),
         gender = as_factor(gender),
         is_male = as_factor(is_male), # should we delete this?
         ethnicity = as_factor(ethnicity),
         race_white = as_factor(race_white),
         race_black = as_factor(race_black),
         race_hispanic = as_factor(race_hispanic),
         race_other = as_factor(race_other),
         metastatic_cancer = as_factor(metastatic_cancer),
         diabetes = as_factor(diabetes),
         first_service = as_factor(first_service),
         hospital_expire_flag = as_factor(hospital_expire_flag),
         thirtyday_expire_flag = as_factor(thirtyday_expire_flag),
         sepsis_angus = as_factor(sepsis_angus),
         sepsis_martin = as_factor(sepsis_martin),
         sepsis_explicit = as_factor(sepsis_explicit),
         septic_shock_explicit = as_factor(septic_shock_explicit),
         severe_sepsis_explicit = as_factor(severe_sepsis_explicit),
         sepsis_nqf = as_factor(sepsis_nqf),
         sepsis_cdc = as_factor(sepsis_cdc),
         sepsis_cdc_simple = as_factor(sepsis_cdc_simple),
         elixhauser_hospital = as_factor(elixhauser_hospital), # what does this mean?
         vent = as_factor(vent),
         # sofa
         # lods
         # sirs
         # qsofa
         qsofa_sysbp_score = as_factor(qsofa_sysbp_score),
         qsofa_gcs_score = as_factor(qsofa_gcs_score),
         qsofa_resprate_score = as_factor(qsofa_resprate_score),
         rrt = as_factor(rrt)
         # colloid_bolus has 4050 NAs!!! n = 4559!!!
         # crystalloid_bolus has 1194 NAs
         # icustayid
         # hadm_id
         # subject_id
  )



```

# Article Analysis Reproduction 

## Statistical Summaries of Variables

```{r}

# Statistical analysis steps
# Normally distributed continuous variables were
# summarized as the mean ± SD
# Continuous variables of normal distribution were tested by Kolmogorov–Smirnov test
# Non-normally distributed continuous variables were summarized as the median
# Student’s t test, One-way ANOVA, Mann–Whitney U or Kruskal–Wallis H test were used to compare continuous data of non-normally distribution, if appropriate.
# Categorical variables were expressed as numbers or percentage and assessed using Chi-square test or Fisher’s exact test according to different sample sizes as proper
# Fisher’s uses smaller sample sizes: 20 or 30?

```

#### Table 1
Baseline characteristics, vital signs, laboratory parameters and statistic results of mimic-III patients with sepsis. 

```{r}

```

#### Figure 2

##### Figure 2a
Overall ethnicity characteristics pie chart

```{r}

```

##### Figure 2b
The common sources of infection pie chart

```{r}

```

## Three Predictive Models

### Conventional Logistic Regression Model

```{r}

# conducted using these significant variables identified by backward stepwise analysis with Chi-square test. Then we chose an entry probability of < 0.05 by the stepwise selection method

```

#### Table 2
Features selected in the conventional logistic regression 

```{r}

```

### SAPS-II Score Model

```{r}

# we used these time-stamp variables to do prediction based on the methods provided by the original literature of SAPS II [16]

```

### XGBoost Algorithm Model

```{r}

# performed XGBoost model [17, 18] to analysis the contribution (gain) of each variable to 30-days mortality
# at the same time, backward stepwise analysis was processed to select the variable with a threshold of p < 0.05 according to the Akaike information criterion (AIC) [19]
# After identifying the variables through XGBoost, we used these clinical and laboratory variables included to construct the XGBoost algorithm model


# Load required packages
library(tidyverse)
library(xgboost)
library(MASS)       # for stepAIC
library(caret)      # for creating dummy variables and data splitting
library(lubridate)  # if needed for date manipulation

#---------------------------
# Data Preparation
#---------------------------

# We will start with data_numeric, and adjust as necessary

# Exclude non-predictive columns like IDs, timestamps, or any that are known after outcome.
nonpredictor_cols <- c("intime", "outtime", "dbsource",
                       "suspected_infection_time_poe", "antibiotic_time_poe", "blood_culture_time",
                       "hospital_expire_flag", # If they died in the hospital, they died within 30 days, so this doesn't make sense to include
                       "subject_id", "icustay_id", "hadm_id")

# Exclude redundant variables that used to be factors or are not appropriate for this analysis
exclude_cols <- c("gender", # This has already been one hot encoded to is_male 1/0
                  "ethnicity", # has also been one hot encoded
                  "elixhauser_hospital",
                  "first_service", # has 13 levels but not one hot encoded so excluded
                  "specimen_poe", # has 35 levels but also not one hot encoded
                  "colloid_bolus",
                  "crystalloid_bolus",
                  "hosp_los", "icu_los", "lods"
                  )

# Remove columns
model_data <- data_numeric %>%
  select(-all_of(exclude_cols)) %>% 
  select(-all_of(nonpredictor_cols)) 

#---------------------------
# Train-Test Split
#---------------------------

# Split the data into training and test sets
set.seed(1248)
train_index <- createDataPartition(model_data$thirtyday_expire_flag, p = 0.7, list = FALSE)
train_data <- model_data[train_index, ]
test_data  <- model_data[-train_index, ]

# Separate features and labels
x_train <- train_data %>% select(-thirtyday_expire_flag)
y_train <- train_data %>% select(thirtyday_expire_flag)
#train_y <- train_data$thirtyday_expire_flag

x_test <- test_data %>% select(-thirtyday_expire_flag)
y_test <- test_data %>% select(thirtyday_expire_flag)

#---------------------------
# XGBoost Model for Feature Importance
#---------------------------

# Convert to xgb.DMatrix
dtrain <- xgb.DMatrix(data = as.matrix(x_train), label = as.matrix(y_train))
dtest <- xgb.DMatrix(data = as.matrix(x_test), label = as.matrix(y_test))

# Basic parameters for XGBoost - these can be tuned
params <- list(
  objective = "binary:logistic",
  eval_metric = "auc",
  eta = 0.1,
  max_depth = 6,
  min_child_weight = 10,
  subsample = 0.8,
  colsample_bytree = 0.8
)

# Train the model
xgb_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 100,
  watchlist = list(train = dtrain, test = dtest),
  early_stopping_rounds = 10,
  print_every_n = 10 #verbose = 0
)

# Extract feature importance
importance_xg1 <- xgb.importance(model = xgb_model)
head(importance_xg1, 20)  # Top 20 most important features (by Gain)

# Plot feature importance
head(importance_xg1, 20) %>% 
  ggplot(aes(x = fct_reorder(Feature, Gain, .desc = FALSE), y = Gain)) +
    geom_col() + 
    coord_flip() +
    theme_minimal() +
    labs(x = "Gain",
         y = "Feature",
         title = "Original XG Boost Feature Importance")
  

#---------------------------
# Backward Stepwise Selection Using AIC
#---------------------------

# After reviewing the importance, we might select a subset of top variables to consider.
# For demonstration, let's take the top 30 variables by Gain and then run a logistic regression with stepAIC.

# Extract top features
top_vars <- importance_xg1 %>% 
  top_n(50, wt = Gain) %>%
  pull(Feature)

# Filter the dataset to these top variables
# Use data_vartypes because we are going logistic regression so need categorical variables as factors
data_top <- data_vartypes %>% 
  select(thirtyday_expire_flag, all_of(top_vars)) %>% 
#  select(-c("hosp_los", "icu_los", "lods")) %>% 
  na.omit()

# data_top <- na.omit(data_top)


# Fit a full logistic regression model using these top variables
full_lr_model <- glm(thirtyday_expire_flag ~ ., data = data_top, family = binomial)
summary(full_lr_model)

# # Check for multicolinearity in the lr
# car::vif(full_lr_model)
# alias(full_lr_model)
# # Identify aliased variables
# aliased_vars <- rownames(alias(full_lr_model)$Complete)
# # Remove aliased variables from the dataset
# data_top_clean <- data_top %>% select(-all_of(aliased_vars))
# # Refit the logistic regression model
# full_lr_model_clean <- glm(thirtyday_expire_flag ~ ., data = data_top_clean, family = binomial)
# # Calculate VIF for the cleaned model
# vif_values <- car::vif(full_lr_model_clean)
# #print(vif_values)




# Perform backward stepwise selection based on AIC
stepwise_lr_model <- stepAIC(full_lr_model, direction = "backward", trace = FALSE)

# Check the summary of the final model
summary(stepwise_lr_model)

###############################
# I'm not sure if here I should only keep the statistically significant ones
###############################

# The summary will show which variables remain. We can select those final variables:
final_vars <- names(coef(stepwise_lr_model))[-1]  # remove the intercept

final_vars

# Remove trailing '1' from variable names dynamically
final_vars <- str_remove(final_vars, "1$")
print(final_vars)


# ---------------------------------------------------------
# Refitting XGBoost With Selected Variables
# ---------------------------------------------------------

# Extract the first column dynamically using dplyr
y_test <- y_test %>% pull(1)

# Use only the selected variables
train_x_selected <- train_data %>% select(all_of(final_vars))
test_x_selected  <- test_data %>% select(all_of(final_vars))

dtrain_selected <- xgb.DMatrix(data = as.matrix(train_x_selected), 
                               label = as.matrix(y_train))
dtest_selected  <- xgb.DMatrix(data = as.matrix(test_x_selected), 
                               label = as.matrix(y_test))

# Retrain XGBoost model with the selected variables
set.seed(123)
xgb_final_model <- xgb.train(
  params = params,
  data = dtrain_selected,
  nrounds = 100,
  watchlist = list(train = dtrain_selected, test = dtest_selected),
  early_stopping_rounds = 10,
  verbose = 0
)

# Evaluate final model performance
xg_preds <- predict(xgb_final_model, dtest_selected)

library(pROC)
roc_obj <- pROC::roc(response = y_test, # y_test <- y_test %>% pull(1)
                     predictor = xg_preds)
auc_val <- pROC::auc(roc_obj)
cat("Final model AUC:", auc_val, "\n")

# Plot the ROC curve
plot(roc_obj, col = "blue", main = paste0("Final XGBoost ROC Curve (AUC = ", round(auc(roc_obj),4) , ")"))

# Feature importance on the reduced model
final_importance <- xgb.importance(feature_names = colnames(train_x_selected), model = xgb_final_model)
print(final_importance)

# Plot feature importance
head(final_importance, 20) %>% 
  ggplot(aes(x = fct_reorder(Feature, Gain, .desc = FALSE), y = Gain)) +
    geom_col() + 
    coord_flip() +
    theme_minimal() +
    labs(x = "Gain",
         y = "Feature",
         title = "Final XG Boost Feature Importance")


```

#### Table 3
Features selected in the XGboost model

```{r}

```

#### Figure 3
Analysis results of each features’ contribution by XGBoost model 

```{r}

```

## Model Comparison

### ROC/AUC and DCA

```{r}

# Tested and compared the performances of the three predictive models by area under curves (AUCs) of the receiver operating characteristic curves (ROC) and decision curve analysis (DCA), then selected the model that achieved the highest overall diagnostic value for further verification

```

#### Figure 4
ROC curves of the three models

```{r}

```

#### Figure 5
Decision curve analysis (DCA) of the three prediction models

```{r}

```


### Nomogram and Clinical Impact Curve (CIC) 

```{r}
# At last, nomogram and clinical impact curve (CIC) were plotted to evaluate the clinical usefulness and applicability net benefits of the model with the best diagnostic value
```

#### Figure 6
Nomogram to estimate the risk of mortality in sepsis patients (for visualization of the XGboost predictive model)

```{r}

```

#### Figure 7
Clinical impact curve (CIC) of XGboost model 

```{r}

```


# Additional Analyses

# Compare Original to Additional Analysis Methods

