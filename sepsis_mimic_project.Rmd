---
title: "ML MIMIC Sepsis Project"
author: "Christine Lucille Kuryla"
date: "2024-12-07"
output: 
  html_document:
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)


library(tidyverse)
library(xgboost)
library(MASS)       # for stepAIC
library(caret)      # for creating dummy variables and data splitting
library(lubridate)  # if needed for date manipulation
library(rmda)  # For decision curve analysis

library(conflicted)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")


```

# Article

Hou, N., Li, M., He, L. et al. Predicting 30-days mortality for MIMIC-III patients with sepsis-3: a machine learning approach using XGboost. J Transl Med 18, 462 (2020). https://doi.org/10.1186/s12967-020-02620-5

https://translational-medicine.biomedcentral.com/articles/10.1186/s12967-020-02620-5 

# Data from online article

## Load Data
```{r load_data}
# Downloaded from: https://translational-medicine.biomedcentral.com/articles/10.1186/s12967-020-02620-5#Sec14

# This data has all of the patients included in the study, after they filtered out certain patients due to the exclusion criteria in the paper
# n = 4559

data <- read_csv("data/12967_2020_2620_MOESM1_ESM.csv") 

# colnames(data)

# Remove duplicates

# all(data$icustay_id...1 == data$icustay_id...103) # TRUE
# all(data$hadm_id...102 == data$hadm_id...2) # TRUE

data <- data %>% 
  mutate(icustay_id = icustay_id...1,
         hadm_id = hadm_id...2
         ) %>% 
  select(-c("icustay_id...1", "icustay_id...103",
            "hadm_id...2", "hadm_id...102"
            ))

# Some methods need numeric data, some need different variable types. Let's create two dataframes.

data_numeric <- data

# Convert into appropriate data types

data_vartypes <- data %>% 
  mutate(intime = dmy_hms(intime),
         outtime = dmy_hms(outtime),
         dbsource = as_factor(dbsource),
         suspected_infection_time_poe = dmy_hms(suspected_infection_time_poe),
         specimen_poe = as_factor(specimen_poe),
         positiveculture_poe = as_factor(positiveculture_poe),
         antibiotic_time_poe = dmy_hms(antibiotic_time_poe),
         blood_culture_time = dmy_hms(blood_culture_time),
         blood_culture_positive = as_factor(blood_culture_positive),
         gender = as_factor(gender),
         is_male = as_factor(is_male), # should we delete this?
         ethnicity = as_factor(ethnicity),
         race_white = as_factor(race_white),
         race_black = as_factor(race_black),
         race_hispanic = as_factor(race_hispanic),
         race_other = as_factor(race_other),
         metastatic_cancer = as_factor(metastatic_cancer),
         diabetes = as_factor(diabetes),
         first_service = as_factor(first_service),
         hospital_expire_flag = as_factor(hospital_expire_flag),
         thirtyday_expire_flag = as_factor(thirtyday_expire_flag),
         sepsis_angus = as_factor(sepsis_angus),
         sepsis_martin = as_factor(sepsis_martin),
         sepsis_explicit = as_factor(sepsis_explicit),
         septic_shock_explicit = as_factor(septic_shock_explicit),
         severe_sepsis_explicit = as_factor(severe_sepsis_explicit),
         sepsis_nqf = as_factor(sepsis_nqf),
         sepsis_cdc = as_factor(sepsis_cdc),
         sepsis_cdc_simple = as_factor(sepsis_cdc_simple),
         elixhauser_hospital = as_factor(elixhauser_hospital), # what does this mean?
         vent = as_factor(vent),
         # sofa
         # lods
         # sirs
         # qsofa
         qsofa_sysbp_score = as_factor(qsofa_sysbp_score),
         qsofa_gcs_score = as_factor(qsofa_gcs_score),
         qsofa_resprate_score = as_factor(qsofa_resprate_score),
         rrt = as_factor(rrt)
         # colloid_bolus has 4050 NAs!!! n = 4559!!!
         # crystalloid_bolus has 1194 NAs
         # icustayid
         # hadm_id
         # subject_id
  )



```

# Article Analysis Reproduction 

## Statistical Summaries of Variables

```{r}

# Statistical analysis steps
# Normally distributed continuous variables were
# summarized as the mean ± SD
# Continuous variables of normal distribution were tested by Kolmogorov–Smirnov test
# Non-normally distributed continuous variables were summarized as the median
# Student’s t test, One-way ANOVA, Mann–Whitney U or Kruskal–Wallis H test were used to compare continuous data of non-normally distribution, if appropriate.
# Categorical variables were expressed as numbers or percentage and assessed using Chi-square test or Fisher’s exact test according to different sample sizes as proper
# Fisher’s uses smaller sample sizes: 20 or 30?

```

#### Table 1
Baseline characteristics, vital signs, laboratory parameters and statistic results of mimic-III patients with sepsis. 

```{r}

```

#### Figure 2

##### Figure 2a
Overall ethnicity characteristics pie chart

```{r}

```

##### Figure 2b
The common sources of infection pie chart

```{r}

```

## Three Predictive Models

### Conventional Logistic Regression Model

```{r}

# conducted using these significant variables identified by backward stepwise analysis with Chi-square test. Then we chose an entry probability of < 0.05 by the stepwise selection method

# Load required packages
library(tidyverse)
library(MASS)       # for stepAIC
library(caret)      # for creating dummy variables and data splitting

#---------------------------
# Data Preparation
#---------------------------

# Start with your data, excluding non-predictive or redundant columns
nonpredictor_cols <- c("intime", "outtime", "dbsource",
                       "suspected_infection_time_poe", "antibiotic_time_poe", "blood_culture_time",
                       "hospital_expire_flag", # If they died in the hospital, they died within 30 days, so this doesn't make sense to include
                       "subject_id", "icustay_id", "hadm_id")

exclude_cols <- c("gender", "ethnicity", "elixhauser_hospital",
                  "first_service", "specimen_poe", "colloid_bolus",
                  "crystalloid_bolus", "hosp_los", "icu_los", "lods")

# Remove columns
model_data <- data_vartypes %>%
  select(-all_of(exclude_cols)) %>% 
  select(-all_of(nonpredictor_cols)) 

#---------------------------
# Train-Test Split
#---------------------------

# Split the data into training and test sets
set.seed(1248)
train_index <- createDataPartition(model_data$thirtyday_expire_flag, p = 0.7, list = FALSE)
train_data <- model_data[train_index, ]
test_data  <- model_data[-train_index, ]

# Remove rows with missing values
train_data <- na.omit(train_data)

# Double-check
sum(is.na(train_data))  # Should return 0


#---------------------------
# Full Logistic Regression Model
#---------------------------

# Fit the full logistic regression model
full_lr_model <- glm(thirtyday_expire_flag ~ ., data = train_data, family = binomial)

# Summarize the full model
summary(full_lr_model)

#---------------------------
# Backward Stepwise Selection Using AIC
#---------------------------

# Perform backward stepwise selection
stepwise_lr_model <- stepAIC(full_lr_model, direction = "backward", trace = FALSE)

# Check the summary of the final model
summary(stepwise_lr_model)

# Extract the final variables
final_vars <- names(coef(stepwise_lr_model))[-1]  # Remove intercept
print(final_vars)
# Remove trailing '1' from variable names dynamically
final_vars <- str_remove(final_vars, "1$")
print(final_vars)
#---------------------------
# Final Logistic Regression Model
#---------------------------

# Refit logistic regression using only the selected variables
final_train_data <- train_data %>% select(thirtyday_expire_flag, all_of(final_vars))
final_test_data <- test_data %>% select(thirtyday_expire_flag, all_of(final_vars))

final_lr_model <- glm(thirtyday_expire_flag ~ ., data = final_train_data, family = binomial)

# Summarize the final model
summary(final_lr_model)
```

```{r}
#---------------------------
# Model Evaluation
#---------------------------

# Predict on the test set
pred_probs <- predict(final_lr_model, newdata = final_test_data, type = "response")
final_test_data$pred <- pred_probs

# Convert probabilities to binary outcomes
threshold <- 0.5
pred_classes <- ifelse(pred_probs > threshold, 1, 0)

# Confusion Matrix
conf_matrix <- table(Predicted = pred_classes, Actual = final_test_data$thirtyday_expire_flag)
print(conf_matrix)

# Calculate accuracy, sensitivity, and specificity
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Accuracy:", accuracy, "\n")

# Calculate AUC using pROC
library(pROC)
roc_obj <- roc(final_test_data$thirtyday_expire_flag, pred_probs)
auc_val <- auc(roc_obj)
cat("AUC:", auc_val, "\n")

# Plot ROC curve
plot(roc_obj, col = "blue", main = paste("ROC Curve (AUC =", round(auc_val, 4), ")"))


# Impute missing values (e.g., using the median for numeric variables)
final_test_data <- final_test_data %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), median(., na.rm = TRUE), .)))

# Convert to numeric
final_test_data$thirtyday_expire_flag <- as.numeric(as.character(final_test_data$thirtyday_expire_flag))


# Decision curve analysis for the test dataset
dca_result <- decision_curve(
  formula = thirtyday_expire_flag ~ pred,
  data = final_test_data,
  family = binomial(link = "logit"),
  thresholds = seq(0, 1, by = 0.01),
  confidence.intervals = FALSE
)
# Plot decision curve
plot_decision_curve(
  dca_result,
  xlab = "Threshold Probability",
  ylab = "Net Benefit",
  legend.position = "right"
)

#---------------------------
# Plot the Clinical Impact Curve
#---------------------------

# Clinical impact curve for test dataset
plot_clinical_impact(
  dca_result,
  population.size = 100,   # Represent per 100 patients
  xlab = "Threshold Probability",
  ylab = "Number of Patients per 100"
)

```

#### Table 2
Features selected in the conventional logistic regression 

```{r}

```

### SAPS-II Score Model

```{r}

# we used these time-stamp varia
# we used these time-stamp variables to do prediction based on the methods provided by the original literature of SAPS II [16]

# we used these time-stamp variables to do prediction based on the methods provided by the original literature of SAPS II [16]

calculate_saps2 <- function(age, heartrate, sysbp_min, sysbp_max, temp, bun, urineoutput,
                            sodium_min, sodium_max, potassium_min, potassium_max,
                            bicarbonate, wbc_min, wbc_max) {
  age_score <- ifelse(age < 40, 0,
                      ifelse(age >= 40 & age <= 59, 7,
                             ifelse(age >= 60 & age <= 69, 12,
                                    ifelse(age >= 70 & age <= 74, 15,
                                           ifelse(age >= 75, 18, NA)))))
  
  hr_score <- ifelse(heartrate < 40, 11,
                     ifelse(heartrate >= 40 & heartrate <= 69, 2,
                            ifelse(heartrate >= 70 & heartrate <= 119, 0,
                                   ifelse(heartrate >= 120 & heartrate <= 159, 4,
                                          ifelse(heartrate >= 160, 7, NA)))))
  
  worst_sbp <- pmin(sysbp_min, sysbp_max, na.rm = TRUE)
  sbp_score <- ifelse(worst_sbp < 70, 13,
                      ifelse(worst_sbp >= 70 & worst_sbp <= 99, 5,
                             ifelse(worst_sbp >= 100 & worst_sbp <= 199, 0,
                                    ifelse(worst_sbp >= 200, 2, NA))))
  
  temp_score <- ifelse(temp < 39, 0, 
                       ifelse(temp >= 39, 3, NA))
  
  bun_score <- ifelse(bun < 28, 0,
                      ifelse(bun >= 28 & bun <= 83, 6,
                             ifelse(bun > 83, 10, NA)))
  
  urine_score <- ifelse(urineoutput >= 1000, 0,
                        ifelse(urineoutput >= 500 & urineoutput < 1000, 4,
                               ifelse(urineoutput < 500, 11, NA)))
  
  worst_sodium <- pmin(sodium_min, sodium_max, na.rm = TRUE)
  sodium_score <- ifelse(worst_sodium < 125, 5,
                         ifelse(worst_sodium >= 125 & worst_sodium <= 144, 0,
                                ifelse(worst_sodium > 144, 1, NA)))
  
  worst_potassium <- pmax(potassium_min, potassium_max, na.rm = TRUE)
  potassium_score <- ifelse(worst_potassium < 3, 3,
                            ifelse(worst_potassium >= 3 & worst_potassium <= 5.4, 0,
                                   ifelse(worst_potassium > 5.4, 3, NA)))
  
  bicarbonate_score <- ifelse(bicarbonate >= 20, 0,
                              ifelse(bicarbonate < 20, 6, NA))
  
  worst_wbc <- pmin(wbc_min, wbc_max, na.rm = TRUE)
  wbc_score <- ifelse(worst_wbc < 1, 12,
                      ifelse(worst_wbc >= 1 & worst_wbc <= 19.9, 0,
                             ifelse(worst_wbc > 19.9, 3, NA)))
  
  total_score <- rowSums(cbind(age_score, hr_score, sbp_score, temp_score, bun_score, 
                               urine_score, sodium_score, potassium_score, 
                               bicarbonate_score, wbc_score), na.rm = TRUE)
  
  return(total_score)
}

saps2_score <- calculate_saps2(data$age, data$heartrate_mean, data$sysbp_min, 
                                    data$sysbp_max, data$tempc_max, data$bun_max, 
                                    data$urineoutput, data$sodium_min, data$sodium_max, 
                                    data$potassium_min, data$potassium_max, 
                                    data$bicarbonate_min, data$wbc_min, data$wbc_max)

threshold <- 35  # check 

saps_predicted_mortality <- ifelse(saps2_score > threshold, 1, 0)  # 1 = Predicted to die, 0 = Predicted to survive

saps_confusion_matrix <- table(Predicted = saps_predicted_mortality, Actual = data$hospital_expire_flag)

print("Confusion Matrix:")
print(saps_confusion_matrix)

saps_true_positive <- saps_confusion_matrix[2, 2]  # Predicted to die and actually died
saps_false_positive <- saps_confusion_matrix[2, 1]  # Predicted to die but survived
saps_true_negative <- saps_confusion_matrix[1, 1]  # Predicted to survive and actually survived
saps_false_negative <- saps_confusion_matrix[1, 2]  # Predicted to survive but died

saps_accuracy <- (saps_true_positive + saps_true_negative) / sum(saps_confusion_matrix)
saps_precision <- saps_true_positive / (saps_true_positive + saps_false_positive)
saps_recall <- saps_true_positive / (saps_true_positive + saps_false_negative)
saps_f1_score <- 2 * (saps_precision * saps_recall) / (saps_precision + saps_recall)

cat("Accuracy:", saps_accuracy, "\n")
cat("Precision:", saps_precision, "\n")
cat("Recall (Sensitivity):", saps_recall, "\n")
cat("F1-Score:", saps_f1_score, "\n")

library(pROC)

saps_thirtyday_expire_flag <- as.numeric(data$thirtyday_expire_flag)

saps_roc_curve <- roc(saps_thirtyday_expire_flag, saps2_score)

plot(saps_roc_curve, col = "blue", main = "ROC Curve for SAPS II Predictions")
abline(a = 0, b = 1, lty = 2, col = "red")  

saps_auc_value <- auc(saps_roc_curve)
cat("Area Under the Curve (AUC):", saps_auc_value, "\n")

```

### XGBoost Algorithm Model

```{r}

# performed XGBoost model [17, 18] to analysis the contribution (gain) of each variable to 30-days mortality
# at the same time, backward stepwise analysis was processed to select the variable with a threshold of p < 0.05 according to the Akaike information criterion (AIC) [19]
# After identifying the variables through XGBoost, we used these clinical and laboratory variables included to construct the XGBoost algorithm model


# Load required packages
library(tidyverse)
library(xgboost)
library(MASS)       # for stepAIC
library(caret)      # for creating dummy variables and data splitting
library(lubridate)  # if needed for date manipulation
library(rmda)       # for decision curve analysis

#---------------------------
# Data Preparation
#---------------------------

# We will start with data_numeric, and adjust as necessary

# Exclude non-predictive columns like IDs, timestamps, or any that are known after outcome.
nonpredictor_cols <- c("intime", "outtime", "dbsource",
                       "suspected_infection_time_poe", "antibiotic_time_poe", "blood_culture_time",
                       "hospital_expire_flag", # If they died in the hospital, they died within 30 days, so this doesn't make sense to include
                       "subject_id", "icustay_id", "hadm_id")

# Exclude redundant variables that used to be factors or are not appropriate for this analysis
exclude_cols <- c("gender", # This has already been one hot encoded to is_male 1/0
                  "ethnicity", # has also been one hot encoded
                  "elixhauser_hospital",
                  "first_service", # has 13 levels but not one hot encoded so excluded
                  "specimen_poe", # has 35 levels but also not one hot encoded
                  "colloid_bolus",
                  "crystalloid_bolus",
                  "hosp_los", "icu_los", "lods"
                  )

# Remove columns
model_data <- data_numeric %>%
  select(-all_of(exclude_cols)) %>% 
  select(-all_of(nonpredictor_cols)) 

#---------------------------
# Train-Test Split
#---------------------------

# Split the data into training and test sets
set.seed(1248)
train_index <- createDataPartition(model_data$thirtyday_expire_flag, p = 0.7, list = FALSE)
train_data <- model_data[train_index, ]
test_data  <- model_data[-train_index, ]

# Separate features and labels
x_train <- train_data %>% select(-thirtyday_expire_flag)
y_train <- train_data %>% select(thirtyday_expire_flag)
#train_y <- train_data$thirtyday_expire_flag

x_test <- test_data %>% select(-thirtyday_expire_flag)
y_test <- test_data %>% select(thirtyday_expire_flag)

#---------------------------
# XGBoost Model for Feature Importance
#---------------------------

# Convert to xgb.DMatrix
dtrain <- xgb.DMatrix(data = as.matrix(x_train), label = as.matrix(y_train))
dtest <- xgb.DMatrix(data = as.matrix(x_test), label = as.matrix(y_test))

# Basic parameters for XGBoost - these can be tuned
params <- list(
  objective = "binary:logistic",
  eval_metric = "auc",
  eta = 0.1,
  max_depth = 6,
  min_child_weight = 10,
  subsample = 0.8,
  colsample_bytree = 0.8
)

# Train the model
xgb_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 100,
  watchlist = list(train = dtrain, test = dtest),
  early_stopping_rounds = 10,
  print_every_n = 10 #verbose = 0
)

# Extract feature importance
importance_xg1 <- xgb.importance(model = xgb_model)
head(importance_xg1, 20)  # Top 20 most important features (by Gain)

# Plot feature importance
head(importance_xg1, 20) %>% 
  ggplot(aes(x = fct_reorder(Feature, Gain, .desc = FALSE), y = Gain)) +
    geom_col() + 
    coord_flip() +
    theme_minimal() +
    labs(x = "Gain",
         y = "Feature",
         title = "Original XG Boost Feature Importance")
  

#---------------------------
# Backward Stepwise Selection Using AIC
#---------------------------

# After reviewing the importance, we might select a subset of top variables to consider.
# For demonstration, let's take the top 30 variables by Gain and then run a logistic regression with stepAIC.

# Extract top features
top_vars <- importance_xg1 %>% 
  top_n(50, wt = Gain) %>%
  pull(Feature)

# Filter the dataset to these top variables
# Use data_vartypes because we are going logistic regression so need categorical variables as factors
data_top <- data_vartypes %>% 
  select(thirtyday_expire_flag, all_of(top_vars)) %>% 
#  select(-c("hosp_los", "icu_los", "lods")) %>% 
  na.omit()

# data_top <- na.omit(data_top)


# Fit a full logistic regression model using these top variables
full_lr_model <- glm(thirtyday_expire_flag ~ ., data = data_top, family = binomial)
summary(full_lr_model)

# # Check for multicolinearity in the lr
# car::vif(full_lr_model)
# alias(full_lr_model)
# # Identify aliased variables
# aliased_vars <- rownames(alias(full_lr_model)$Complete)
# # Remove aliased variables from the dataset
# data_top_clean <- data_top %>% select(-all_of(aliased_vars))
# # Refit the logistic regression model
# full_lr_model_clean <- glm(thirtyday_expire_flag ~ ., data = data_top_clean, family = binomial)
# # Calculate VIF for the cleaned model
# vif_values <- car::vif(full_lr_model_clean)
# #print(vif_values)


# Perform backward stepwise selection based on AIC
stepwise_lr_model <- stepAIC(full_lr_model, direction = "backward", trace = FALSE)

# Check the summary of the final model
summary(stepwise_lr_model)

###############################
# I'm not sure if here I should only keep the statistically significant ones
###############################

# The summary will show which variables remain. We can select those final variables:
final_vars <- names(coef(stepwise_lr_model))[-1]  # remove the intercept

final_vars

# Remove trailing '1' from variable names dynamically
final_vars <- str_remove(final_vars, "1$")
print(final_vars)


# ---------------------------------------------------------
# Refitting XGBoost With Selected Variables
# ---------------------------------------------------------

# Extract the first column dynamically using dplyr
y_test <- y_test %>% pull(1)

# Use only the selected variables
train_x_selected <- train_data %>% select(all_of(final_vars))
test_x_selected  <- test_data %>% select(all_of(final_vars))

dtrain_selected <- xgb.DMatrix(data = as.matrix(train_x_selected), 
                               label = as.matrix(y_train))
dtest_selected  <- xgb.DMatrix(data = as.matrix(test_x_selected), 
                               label = as.matrix(y_test))

# Retrain XGBoost model with the selected variables
set.seed(123)
xgb_final_model <- xgb.train(
  params = params,
  data = dtrain_selected,
  nrounds = 100,
  watchlist = list(train = dtrain_selected, test = dtest_selected),
  early_stopping_rounds = 10,
  verbose = 0
)

# Evaluate final model performance
xg_preds <- predict(xgb_final_model, dtest_selected)

# Add predictions to the data frame
# test_data$pred <- xg_preds
# test_data$pred <- predict(xgb_final_model, dtest_selected)

library(pROC)
roc_obj <- pROC::roc(response = y_test, # y_test <- y_test %>% pull(1)
                     predictor = xg_preds)
auc_val <- pROC::auc(roc_obj)
cat("Final model AUC:", auc_val, "\n")

# Plot the ROC curve
plot(roc_obj, col = "blue", main = paste0("Final XGBoost ROC Curve (AUC = ", round(auc(roc_obj),4) , ")"))

# Feature importance on the reduced model
final_importance <- xgb.importance(feature_names = colnames(train_x_selected), model = xgb_final_model)
print(final_importance)

# Plot feature importance
head(final_importance, 20) %>% 
  ggplot(aes(x = fct_reorder(Feature, Gain, .desc = FALSE), y = Gain)) +
    geom_col() + 
    coord_flip() +
    theme_minimal() +
    labs(x = "Gain",
         y = "Feature",
         title = "Final XG Boost Feature Importance")


```

#### Table 3
Features selected in the XGboost model

```{r}

```

#### Figure 3
Analysis results of each features’ contribution by XGBoost model 

```{r}

```

## Model Comparison

### ROC/AUC and DCA

```{r}

# Tested and compared the performances of the three predictive models by area under curves (AUCs) of the receiver operating characteristic curves (ROC) and decision curve analysis (DCA), then selected the model that achieved the highest overall diagnostic value for further verification


# 
# # Identify best model by AUC
# best_model <- metrics_table %>%
#   arrange(desc(mean_roc_auc)) %>%
#   slice(1) %>%
#   pull(model)
# 
# best_spec <- model_specs[[best_model]]
# 
# final_wf <- workflow() %>%
#   add_recipe(rec) %>%
#   add_model(best_spec) %>%
#   fit(data = data)
# 
# # Get predicted probabilities from the final model
# final_preds <- predict(final_wf, new_data = data, type = "prob") %>%
#   bind_cols(data) %>%
#   rename(pred = .pred_1)

# Generate predicted probabilities
# train_data$pred <- predict(xgb_final_model, dtrain)
test_data$pred <- predict(xgb_final_model, dtest_selected)

#---------------------------
# Perform Decision Curve Analysis (DCA)
#---------------------------

# Decision curve analysis for the test dataset
dca_result <- decision_curve(
  formula = thirtyday_expire_flag ~ pred,
  data = test_data,
  family = binomial(link = "logit"),
  thresholds = seq(0, 1, by = 0.01),
  confidence.intervals = FALSE
)
# Plot decision curve
plot_decision_curve(
  dca_result,
  xlab = "Threshold Probability",
  ylab = "Net Benefit",
  legend.position = "right"
)

#---------------------------
# Plot the Clinical Impact Curve
#---------------------------

# Clinical impact curve for test dataset
plot_clinical_impact(
  dca_result,
  population.size = 100,   # Represent per 100 patients
  xlab = "Threshold Probability",
  ylab = "Number of Patients per 100"
)

```

#### Figure 4
ROC curves of the three models

```{r}

```

#### Figure 5
Decision curve analysis (DCA) of the three prediction models

```{r}

```


### Nomogram and Clinical Impact Curve (CIC) 

```{r}
# At last, nomogram and clinical impact curve (CIC) were plotted to evaluate the clinical usefulness and applicability net benefits of the model with the best diagnostic value
```

#### Figure 6
Nomogram to estimate the risk of mortality in sepsis patients (for visualization of the XGboost predictive model)

```{r}

```

#### Figure 7
Clinical impact curve (CIC) of XGboost model 

```{r}

```


# Additional Analyses

#### LDA

```{r}
# Load required libraries
library(MASS)  # For LDA
library(caret) # For train-test split and evaluation
library(pROC)  # For ROC analysis
library(rmda)  # For decision curve analysis

#---------------------------
# Data Preparation
#---------------------------

# Assume your data is already cleaned as `final_test_data` and `final_train_data` 
# and includes numeric features and the binary outcome variable `thirtyday_expire_flag`.

# Ensure the outcome is a factor (LDA requires a categorical target)
final_train_data$thirtyday_expire_flag <- as.factor(final_train_data$thirtyday_expire_flag)
final_test_data$thirtyday_expire_flag <- as.factor(final_test_data$thirtyday_expire_flag)

# Check class distribution
table(final_train_data$thirtyday_expire_flag)

#---------------------------
# Train the LDA Model
#---------------------------

# Fit the LDA model
lda_model <- lda(thirtyday_expire_flag ~ ., data = final_train_data)

# Print model summary
print(lda_model)

#---------------------------
# Predict on Test Data
#---------------------------

# Predict probabilities and classes
lda_predictions <- predict(lda_model, final_test_data)
final_test_data$pred <- lda_predictions$posterior[, 2]  # Probability of class "1"
final_test_data$pred_class <- lda_predictions$class    # Predicted class labels

#---------------------------
# Evaluate Model Performance
#---------------------------

# # Confusion Matrix
# conf_matrix <- table(
#   Predicted = final_test_data$pred_class,
#   Actual = final_test_data$thirtyday_expire_flag
# )
# print(conf_matrix)
# 
# # Calculate accuracy
# accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
# cat("Accuracy:", accuracy, "\n")
# 
# # Sensitivity and Specificity
# sensitivity <- conf_matrix[2, 2] / sum(conf_matrix[, 2])
# specificity <- conf_matrix[1, 1] / sum(conf_matrix[, 1])
# cat("Sensitivity:", sensitivity, "\n")
# cat("Specificity:", specificity, "\n")

#---------------------------
# ROC and AUC
#---------------------------

# Calculate ROC and AUC
roc_obj <- roc(as.numeric(final_test_data$thirtyday_expire_flag) - 1, final_test_data$pred)
auc_val <- auc(roc_obj)
cat("AUC:", auc_val, "\n")

# Plot the ROC Curve
plot(roc_obj, col = "blue", main = paste("ROC Curve (AUC =", round(auc_val, 4), ")"))

#---------------------------
# Decision Curve Analysis (DCA)
#---------------------------

# Convert to numeric
final_test_data$thirtyday_expire_flag <- as.numeric(as.character(final_test_data$thirtyday_expire_flag))

# Perform Decision Curve Analysis
dca_result <- decision_curve(
  formula = thirtyday_expire_flag ~ pred,
  data = final_test_data,
  family = binomial(link = "logit"),
  thresholds = seq(0, 1, by = 0.01),
  confidence.intervals = FALSE
)
# Plot decision curve
plot_decision_curve(
  dca_result,
  xlab = "Threshold Probability",
  ylab = "Net Benefit",
  legend.position = "right"
)

# Plot Clinical Impact Curve
plot_clinical_impact(
  dca_result,
  population.size = 100,
  xlab = "Threshold Probability",
  ylab = "Number of Patients per 100"
)

# Save the Clinical Impact Curve
# ggsave("clinical_impact_curve_lda.png")

```

#### Forward Stepwise XGBoost 

```{r}
# Load required libraries
library(tidyverse)
library(xgboost)
library(MASS)       # For stepAIC
library(caret)      # For data splitting
library(pROC)       # For ROC curve
library(ggplot2)

#---------------------------
# Data Preparation
#---------------------------
nonpredictor_cols <- c("intime", "outtime", "dbsource",
                       "suspected_infection_time_poe", "antibiotic_time_poe", "blood_culture_time",
                       "hospital_expire_flag", "subject_id", "icustay_id", "hadm_id")

exclude_cols <- c("gender", "ethnicity", "elixhauser_hospital",
                  "first_service", "specimen_poe", "colloid_bolus",
                  "crystalloid_bolus", "hosp_los", "icu_los", "lods")

# Filter data
model_data <- data_numeric %>%
  select(-all_of(nonpredictor_cols)) %>%
  select(-all_of(exclude_cols))

# Train-test split
set.seed(1248)
train_index <- createDataPartition(model_data$thirtyday_expire_flag, p = 0.7, list = FALSE)
train_data <- model_data[train_index, ]
test_data  <- model_data[-train_index, ]

x_train <- train_data %>% select(-thirtyday_expire_flag)
y_train <- train_data$thirtyday_expire_flag

x_test <- test_data %>% select(-thirtyday_expire_flag)
y_test <- test_data$thirtyday_expire_flag

#---------------------------
# XGBoost for Feature Importance
#---------------------------
dtrain <- xgb.DMatrix(data = as.matrix(x_train), label = y_train)
dtest  <- xgb.DMatrix(data = as.matrix(x_test), label = y_test)

params <- list(
  objective = "binary:logistic",
  eval_metric = "auc",
  eta = 0.1,
  max_depth = 6,
  min_child_weight = 10,
  subsample = 0.8,
  colsample_bytree = 0.8
)

xgb_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 100,
  watchlist = list(train = dtrain, test = dtest),
  early_stopping_rounds = 10,
  verbose = 0
)

# Extract top features
importance_xg1 <- xgb.importance(model = xgb_model)
top_vars <- importance_xg1 %>%
  top_n(50, wt = Gain) %>%
  pull(Feature)

#---------------------------
# Forward Stepwise Selection
#---------------------------

# Filter dataset to include top variables
data_top <- train_data %>%
  select(thirtyday_expire_flag, all_of(top_vars)) %>%
  na.omit()

# Start with an empty model and full model
empty_model <- glm(thirtyday_expire_flag ~ 1, data = data_top, family = binomial)
full_model  <- glm(thirtyday_expire_flag ~ ., data = data_top, family = binomial)

# Forward stepwise selection
forward_stepwise_lr_model <- stepAIC(
  empty_model, 
  scope = list(lower = empty_model, upper = full_model),
  direction = "forward",
  trace = TRUE
)

# Extract selected variables and clean names
xgboost_forward_final_vars <- names(coef(forward_stepwise_lr_model))[-1]
xgboost_forward_final_vars <- gsub("1$", "", xgboost_forward_final_vars)  # Remove trailing '1'

# Print selected variables
print(xgboost_forward_final_vars)

#---------------------------
# Refitting XGBoost with Selected Variables
#---------------------------
forward_train_x_selected <- train_data %>% 
  select(all_of(intersect(xgboost_forward_final_vars, colnames(train_data))))
forward_test_x_selected  <- test_data %>%
  select(all_of(intersect(xgboost_forward_final_vars, colnames(test_data))))

forward_dtrain_selected <- xgb.DMatrix(data = as.matrix(forward_train_x_selected), label = y_train)
forward_dtest_selected  <- xgb.DMatrix(data = as.matrix(forward_test_x_selected), label = y_test)

xgb_forward_final_model <- xgb.train(
  params = params,
  data = forward_dtrain_selected,
  nrounds = 100,
  watchlist = list(train = forward_dtrain_selected, test = forward_dtest_selected),
  early_stopping_rounds = 10,
  verbose = 0
)

# Evaluate final model
forward_xg_preds <- predict(xgb_forward_final_model, forward_dtest_selected)

roc_obj <- roc(response = y_test, predictor = forward_xg_preds)
auc_val <- auc(roc_obj)
cat("Final model AUC:", auc_val, "\n")

# Plot ROC curve
plot(roc_obj, col = "blue", main = paste0("Forward Stepwise XGBoost ROC Curve (AUC = ", round(auc_val, 4), ")"))

# Feature importance for reduced model
forward_final_importance <- xgb.importance(feature_names = colnames(forward_train_x_selected), model = xgb_forward_final_model)
print(forward_final_importance)

# Plot feature importance
head(forward_final_importance, 20) %>%
  ggplot(aes(x = fct_reorder(Feature, Gain, .desc = FALSE), y = Gain)) +
  geom_col() +
  coord_flip() +
  theme_minimal() +
  labs(x = "Gain",
       y = "Feature",
       title = "Forward Stepwise XGBoost Feature Importance")


```

# Compare Original to Additional Analysis Methods

